# æ¨¡å‹å¾®è°ƒè¯´æ˜è¿‡ç¨‹æ–‡æ¡£ï¼ˆä»¥Qwen2.5_7b_instructä½œä¸ºä¾‹å­ï¼‰

## ğŸ¤– 1. ç¯å¢ƒå‡†å¤‡
1.1 Ubuntu
- Python 3.10.16
- Pytorch 2.5.1
- GPU A100(24GB)

1.2 LLaMA-Factoryæ¡†æ¶
```shell
git clone https://github.com/hiyouga/LLaMA-Factory.git  
cd LLaMA-Factory   
pip install -e .  
```
1.3 è®­ç»ƒæ•°æ®é›†å‡†å¤‡  
ä¸¾ä¸ªä¾‹å­ï¼š
å°†è®­ç»ƒæ•°æ®æ”¾åœ¨ `LLaMA-Factory/data`é‡Œé¢  
ç„¶åä¿®æ”¹æ•°æ®é…ç½®æ–‡ä»¶ï¼š `LLaMA-Factory/data/dataset_info.json` 
```shell
"xxx": {
  "file_name": "xxx.json",
  "columns": {
    "prompt": "instruction",
    "query": "input",
    "response": "output"
  }
}
```

## âš™ï¸ 2. æ¨¡å‹å‡†å¤‡
```shell
pip install -U huggingface_hub
pip install huggingface-cli

huggingface-cli download --resume-download Qwen/Qwen2.5-7B-Instruct --local-dir 'è¿™é‡Œè‡ªå·±æŒ‡å®šåœ°å€'

```


## ğŸ› ï¸ 3. æ¨¡å‹å¾®è°ƒ
3.1 å¾®è°ƒæ–¹æ³•ä¸€ï¼šå¯è§†åŒ–å¾®è°ƒ    
[æ–¹æ³•ä¸€](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md)   
3.2 å¾®è°ƒæ–¹æ³•äºŒï¼šé…ç½®æ–‡ä»¶å¾®è°ƒ    
[æ–¹æ³•äºŒ](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html) 

## â¬ 4. æ¨¡å‹æ¨ç†ä¸éƒ¨ç½²  
4.1
LLaMA-Factoryæ¨ç†å¯ä»¥ä½¿ç”¨ `llamafactory-cli chat inference_config.yaml`æˆ–è€…`llamafactory-cli webchat inference_config.yaml`è¿›è¡Œæ¨ç†ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚  

```shell
model_name_or_path: /group_share/models/Qwen2.5-7B-Instruct
adapter_name_or_path: /group_share/LLaMA-Factory/saves/Qwen2.5-7B-Instruct/lora/train_2025-01-07-12-21-07
template: qwen
infer_backend: vllm  # choices: [huggingface, vllm]
trust_remote_code: true
finetuning_type: lora
```   
### 4.2 æŠŠåŸæ¨¡å‹å’ŒLoRAé€‚é…å™¨è¿›è¡Œåˆå¹¶    
ä½¿ç”¨ `llamafactory-cli export merge_config.yaml` æŒ‡ä»¤æ¥åˆå¹¶æ¨¡å‹ã€‚   
```shell
model_name_or_path: /group_share/models/Qwen2.5-7B-Instruct
adapter_name_or_path: /group_share/LLaMA-Factory/saves/Qwen2.5-7B-Instruct/lora/train_2025-01-07-12-21-07
template: qwen
infer_backend: vllm  # choices: [huggingface, vllm]
trust_remote_code: true
finetuning_type: lora

### export
export_dir: /group_share/models/AgriMind_Qwen2.5_7b_instruct_v1
export_size: 2
export_device: cpu
export_legacy_format: false

```    
### 4.3 å°†æ¨¡å‹ä»¥apiå½¢å¼è¿›è¡Œéƒ¨ç½²ï¼ˆå…¶ä»–éƒ¨ç½²å½¢å¼å¯ä»¥çœ‹å®˜æ–¹æ–‡æ¡£ï¼‰  
å¯åŠ¨ç±»ä¼¼ `API_PORT=8000 CUDA_VISIBLE_DEVICES=0 llamafactory-cli api examples/inference/llama3_lora_sft.yaml` æŒ‡ä»¤ï¼Œè¿™é‡Œçš„é…ç½®æ–‡ä»¶æŒ‰è‡ªèº«é…ç½®è¿›è¡Œä¿®æ”¹ã€‚  

å†™ä¸€ä¸ªapi_call_exmple.py æ¥æµ‹è¯•ä¸€ä¸‹   
```shell
# api_call_example.py
from openai import OpenAI 
client = OpenAI(api_key="0",base_url="http://0.0.0.0:8000/v1")
messages = [{"role": "user", "content": "ä¿ºå®¶å…»äº†å‡ å¤´ç‰›ï¼Œæœ€è¿‘å‘ç°å®ƒä»¬é•¿å¾—ä¸å¿«ï¼Œé¥²æ–™ä¹Ÿåƒäº†ä¸å°‘ï¼Œä½†å°±æ˜¯ä¸è§é•¿è‚‰ã€‚ä¿ºå¬è¯´é¥²æ–™é…æ¯”å¾ˆé‡è¦ï¼Œå¯ä¿ºä¸æ‡‚å’‹é…ï¼Œèƒ½ä¸èƒ½æ•™æ•™ä¿ºå’‹å¼„ï¼Ÿ"}]
result = client.chat.completions.create(messages=messages, model="/group_share/models/Qwen2.5-7B-Instruct")
print(result.choices[0].message)
```  

å¯¹åº”åŒæ ·çš„è¾“å…¥   
ä½¿ç”¨é€šç”¨å¤§æ¨¡å‹çš„æ•ˆæœï¼š    
![before_finetuning](normal.png)  


å¾®è°ƒä¹‹åçš„æ•ˆæœï¼šæ›´åŠ äº²åˆ‡ç›´ç™½ï¼Œè·Ÿå†œæ°‘ä¼¯ä¼¯çš„è·ç¦»æ„Ÿæ²¡é‚£ä¹ˆå¼º  
![compare](after_finetuning.png)  