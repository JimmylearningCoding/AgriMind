from code import interact
import os
import base64
from openai import OpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Loading:åŠ è½½ducument
base_dir = '/group_share/LangChain_study/data'
documents = []

for file in os.listdir(base_dir):
    file_path = os.path.join(base_dir,file)
    if file.endswith('.pdf'):
        loader = PyPDFLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.docx'):
        loader = Docx2txtLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.txt'):
        loader = TextLoader(file_path)
        documents.extend(loader.load())

# Splitting:å°†documentåˆ‡åˆ†æˆå—ä»¥ä¾¿åç»­è¿›è¡ŒåµŒå…¥å’Œå‘é‡å­˜å‚¨
text_spliter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=10)
chunked_documents = text_spliter.split_documents(documents)

# Storing:å°†åˆ†å‰²åµŒå…¥å¹¶å­˜å‚¨åœ¨çŸ¢é‡æ•°æ®åº“ä¸­
from langchain_community.vectorstores import Qdrant
from langchain_openai import ChatOpenAI,OpenAIEmbeddings
from langchain_community.embeddings import ZhipuAIEmbeddings


embedding = ZhipuAIEmbeddings(
    model = 'embedding-3',
    api_key = '.' 
)
vectorstore = Qdrant.from_documents(
    documents=chunked_documents, # ä»¥åˆ†å—çš„æ–‡æ¡£
    embedding=embedding, # ç”¨OpenAIçš„Embedding ModelåšåµŒå…¥
    location=":memory:",  # in-memory å­˜å‚¨
    collection_name="my_documents",) # æŒ‡å®šcollection_name

# Retrieval å‡†å¤‡æ¨¡å‹å’ŒRetrievalé“¾
import logging # å¯¼å…¥Loggingå·¥å…·
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetrieverå·¥å…·
from langchain.chains import RetrievalQA # RetrievalQAé“¾

# è®¾ç½®Logging
logging.basicConfig()
logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)

llm = ChatOpenAI(
    model = 'deepseek-chat',
    openai_api_key = 'sk-',
    openai_api_base = 'https://api.deepseek.com',
    max_tokens = 1024
)

retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(),llm=llm)

qa_chain = RetrievalQA.from_chain_type(llm,retriever=retriever_from_llm)
 
# Outputè¿‡ç¨‹
# Gradio: å®šä¹‰ç”¨æˆ·äº¤äº’ç•Œé¢
import gradio as gr
from gradio_client import Client, handle_file
# Gradio: Define the user interface

conversation_history = []

def chat_interface(history,question):
    
    # è®°å½•ç”¨æˆ·æé—®
    history.append(("ç”¨æˆ·",question))

    # æ„å»ºä¸Šä¸‹æ–‡
    conversation = history[-3:]
    conversation_context = "\n".join([f"{role}: {message}" for role, message in conversation])

    response = llm.invoke(conversation_context + "\nç”¨æˆ·: " + question)
    answer = response.content
    
    # è®°å½•æ¨¡å‹å›ç­”
    history.append(("AI",answer))
    return history,"" # è¿”å›æ›´æ–°åçš„å¯¹è¯å†å²ï¼ŒåŒæ—¶æ¸…ç©ºè¾“å…¥æ¡†

def qa_interface(question):
    """
    Handles user query and retrieves the answer using the QA chain.
    """
    result = qa_chain({"query": question})
    return result['result']


def encode_image(image_path):
    with open(image_path,"rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

def qvq_interface(question, image_path):
    if not image_path:
        return 'è¯·ä¸Šä¼ å›¾ç‰‡'
    base64_image = encode_image(image_path)
    client = OpenAI(
        api_key = "sk-",
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    response = client.chat.completions.create(
    model="qwen-vl-max",
    messages = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step."}
            ],
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpg;base64,{base64_image}"}
                    # "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png"}
                },
                # {"type": "text", "text": question},
                {"type": "text", "text": question},
            ],
        }
    ],
    stream=False
    )
    print(response) 
    result = response.choices[0].message.content
    return result 


# Define the Gradio interface with custom CSS
with gr.Blocks(css="""
    .header { text-align: center; font-size: 2.5rem; color: #333; margin-bottom: 20px; }
    .button { background-color: #4CAF50; color: white; padding: 12px 24px; border: none; border-radius: 5px; font-size: 1.2rem; transition: background-color 0.3s ease; }
    .button:hover { background-color: #45a049; }
    .output { background-color: #fff; padding: 15px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); font-size: 1.1rem; margin-top: 20px; color: #333; }
    .footer { text-align: center; color: #777; margin-top: 30px; font-size: 0.9rem; }
    }
""") as demo:
    gr.Markdown("<div class = 'header'> ğŸŒ± AgriMind - æ™ºèƒ½å†œä¸šå¹³å°")
    
    with gr.Tabs():
        with gr.TabItem("ğŸ¤– ç›´æ¥å’Œè¯­è¨€å¤§æ¨¡å‹è¿›è¡ŒèŠå¤©"):
            # gr.Markdown("### ğŸ¤– ç›´æ¥å’Œå¤§æ¨¡å‹è¿›è¡ŒèŠå¤©")
            chat_input = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...", lines=3)
            chat_button = gr.Button("ç”Ÿæˆç­”æ¡ˆ", elem_classes="button")
            # chat_output = gr.Textbox(label="æ¨¡å‹å›ç­”", lines=5, interactive=False)
            chat_output = gr.Chatbot(label="èŠå¤©è®°å½•")
            chat_button.click(chat_interface,inputs=[chat_output,chat_input],outputs=[chat_output,chat_input])
        
        with gr.TabItem("ğŸ˜€ æ–‡æ¡£å›ç­”åŠ©æ‰‹"):
            # gr.Markdown("### ğŸ˜€ æ–‡æ¡£å›ç­”åŠ©æ‰‹")
            doc_input = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæ¯”å¦‚ï¼šè¿™ä»½æ–‡æ¡£çš„é‡ç‚¹æ˜¯ä»€ä¹ˆ", lines=3)
            doc_button = gr.Button("ç”Ÿæˆç­”æ¡ˆ",elem_classes="button")
            doc_output = gr.Textbox(label="æ¨¡å‹å›ç­”", lines=5, interactive=False)
            doc_button.click(qa_interface,inputs=doc_input,outputs=doc_output)

        with gr.TabItem("ğŸ› ï¸ ç›´æ¥å’Œè§†è§‰å¤§æ¨¡å‹è¿›è¡ŒèŠå¤©"):
            with gr.Row():
                with gr.Column():
                    image_input = gr.Image(label="è¯·ä¸Šä¼ å›¾ç‰‡", type="filepath",height=300)
                    chat_qvq_input = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜", lines=3)
                    chat_qvq_button = gr.Button("ç”Ÿæˆå›ç­”", elem_classes="button")
                with gr.Column():
                    chat_qvq_output = gr.Textbox(label="æ¨¡å‹å›ç­”", lines=8, interactive=False)
            chat_qvq_button.click(qvq_interface,inputs=[chat_qvq_input,image_input],outputs=chat_qvq_output)
            

    gr.Markdown("<div class='footer'>ç”± Deepseek Langchain Gradio æä¾›æ”¯æŒ</div>")

# Start the Gradio app
demo.launch(server_name="0.0.0.0", server_port=5000, share=True)